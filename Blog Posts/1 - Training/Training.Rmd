---
title: "Text Generation 1 - Training"
author: "James Otto"
date: "12/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = 'center',
  fig.width = 8,
  out.width = "100%")
```

## Introduction


```{r setup_visible, message = FALSE}
library("tidyverse"); theme_set(theme_bw()); theme_update(panel.grid.minor = element_blank())
library("here")

library("tensorflow")
library("keras")
```

### A (Brief) Introduction to Neural Networks

### A (Briefer) Introduction to Recurrent Neural Networks

***

## Training a Network

For details on the data wrangling process, see the Appendix.

We have formatted the data so that it is ready to be fed into the RNNs we are going to train.
Currently, we have data frames containing all sequences of lengths 1, 5, 10, and 30 for each corpus.
Before fitting models for all corpuses and sequences, we'll walk through the process of training a RNN on sequences of length 5 from Merry Shelley's Frankenstein.

First, we have to do a little processing as tensorflow expects data in the form of matrices:

```{r data, message = FALSE}
df <- read_csv(here("Data/Training_Data/Merry Shelley/df_seq_5.csv")) 

X_mat <- df |>
  select(starts_with("X")) |>
  as.matrix()

Y_mat <- df |>
  select(Y) |>
  as.matrix()

# tensorflow starts enumerating at 0
X_mat <- X_mat - 1
Y_mat <- Y_mat - 1
```

Now, we define the RNN.
Notice, we have an embedding layer before the rnn layer.
The embedding layer "condenses" the high dimensional binary input space representing different letters into
a lower dimensional continuous space with distances learned from the data.
After being mapped into this space, the data is ready for the the recurrent layer.

```{r define_network, message = FALSE}
n_target_nodes <- length(unique(df$Y)) 

model <- keras_model_sequential() |>
  layer_embedding(input_dim = n_target_nodes, output_dim = 16) |>
  layer_simple_rnn(units = 1000, activation = "relu") |>
  layer_dense(units = n_target_nodes, activation = "softmax")

model |> compile(loss = "sparse_categorical_crossentropy", optimizer = "adam", metrics = "accuracy")
```

```{r summary_network}
summary(model)
```

Next, we train the network.
Note, we use a validation split of .1 to compare training and validation loss and accuracy across epochs.

```{r training_nework_show, eval = FALSE}
model |> fit(
    X_mat, Y_mat,
    batch_size = 1000, 
    epochs = 40, 
    validation_split = 0.1)
```

```{r training_network_hide, eval = FALSE, include = FALSE}
history <- model |> fit(
    X_mat, Y_mat,
    batch_size = 1000, 
    epochs = 40, 
    validation_split = 0.1)

write_rds(history, here("Blog Posts/1 - Training/model_examples/history.rds"))
save_model_hdf5(model, here("Blog Posts/1 - Training/model_examples/model_5_blog_ex.h5"))
```

```{r loading_network, include = FALSE}
history <- read_rds(here("Blog Posts/1 - Training/model_examples/history.rds"))
model <- load_model_hdf5(here("Blog Posts/1 - Training/model_examples/model_5_blog_ex.h5"))
```

```{r history_network, echo = FALSE}
plot(history, smooth = FALSE) + 
  geom_line()
```

Although it appears that the optimal number of epochs to train is near 6, 
we found that training on a larger number (around 40) yielded better generated text.
We are not sure why this is the case, we believe it might be due to the fact that accuracy and cross entropy loss
are poor metrics of success for these models.
It is also possible that mild overfitting is necessary to produce coherent output.


***

Finally, we are able to make predictions from input sequences!

```{r eval_network, comment = "", cache = TRUE}
set.seed(1)

# need encoder/decoder
Shelley_data <- read_rds(here("Data/Training_Data/Merry Shelley/data.RDS"))

# Making predictions starting with seed "Frank"
seed <- c("F", "r", "a", "n", "k") |>
  Shelley_data$encoder() |>
  matrix(ncol = 5) 

seed <- seed - 1 # Off-by-one between R and Python

for (i in 1:1000) {
  X <- rev(seed)[1:5]
  X <- rev(X)
  X <- matrix(X, ncol = 5)
  
  probs <- predict(model, X) |>
    as.vector()
  
  choices <- 1:(length(probs)) - 1 # Off-by-one between R and Python
  pred <- sample(x = choices, size = 1, prob = probs) 
  
  seed <- c(seed, pred)
}

seed <- seed + 1 # Off-by-one between R and Python

map_chr(seed, Shelley_data$decoder) |>
  paste(collapse = "") |>
  (\(x) cat("\t", x, sep = ""))()

```

It looks *okay*.
The generated text is not going to pass a Turing test, but it is pretty impressive considering it is being "typed" out 
one character at a time by the RNN.

***

## Training all the Networks

In order to train individual networks for each combination of author and sequences of length 1, 5, 10, and 30
we need to make some concessions -- it is infeasible to optimize hyperparameters for each network.
We train each network for 40 epochs, as we mentioned earlier this number seems to produce the best predictions regardless of the validation accuracy and loss.
<!-- We train each network for 8 epochs. -->
<!-- While this seems inappropriate, we spot checked several author and sequence length combinations -->
<!-- and for each it seemed that the training and validation error diverged after around 8 epochs of training. -->
We also chose to construct each network with the same structure.
This is certainly suboptimal and would be an interesting choice to investigate, the networks with input dimension 1 surely need to have a different number of nodes in the hidden RNN layer than those with input dimension 30.
We also use the default learning rate of .001, batch sizes of 1000, and do not implement any kind of regularization (e.g. dropout).

```{r data_30, include = FALSE, eval = FALSE}
df <- read_csv(here("Data/Training_Data/Jane Austen/df_seq_30.csv")) 

X_mat <- df |>
  select(starts_with("X")) |>
  as.matrix()

Y_mat <- df |>
  select(Y) |>
  as.matrix()

# tensorflow starts index at 0
X_mat <- X_mat - 1
Y_mat <- Y_mat - 1

n_target_nodes <- length(unique(df$Y)) 

model <- keras_model_sequential() |>
  layer_embedding(input_dim = n_target_nodes, output_dim = 16) |>
  # layer_lstm(units = 1000, activation = "tanh") |>
  layer_simple_rnn(units = 1000, activation = "relu") |>
  layer_dense(units = n_target_nodes, activation = "softmax")

model |> compile(loss = "sparse_categorical_crossentropy", optimizer = "adam", metrics = "accuracy")

history <- model |> fit(
    X_mat, Y_mat,
    batch_size = 1000, 
    epochs = 40, 
    validation_split = 0.1)

write_rds(history, here("Blog Posts/1 - Training/model_examples/history_30.rds"))
```

```{r history_network_30, echo = FALSE, eval = FALSE}
history <- read_rds(here("Blog Posts/1 - Training/model_examples/history_30.rds"))

plot(history, smooth = FALSE) + 
  geom_line()
```

We train the networks programmatically, mapping across the data sets.
Notice this is the same code as before, just slightly more general.

```{r source_create_training_data, echo = FALSE}
source(here("R/fit_model.R"), local = knitr::knit_global())
```
```{r show_create_training_data, code=xfun::read_utf8(here::here("R/fit_model.R"))}
```

```{r creating_training_data, eval = FALSE}
expand_grid(author = list.files(here("Data/Gutenberg/raw")), k = c(1, 5, 10, 30)) |>
  mutate(n_epochs = 40) |>
  pwalk(fit_model)
```

Now that all of the networks are trained, we are able to evaluate their predictions.
See (blog post 2) for our results.


