---
title: "Text Generation 2 - Making Predictions"
author: "James Otto"
date: "12/14/2021"
output: html_document
bibliography: ../references.bib 
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = 'center',
  fig.width = 8,
  out.width = "100%")
```

## Introduction

This is the second post in a series on generating text with recurrent neural networks (RNNs).
In the [previous post](https://jamesotto852.github.io/text-generation-1/),
we trained sets of networks based on works by Mary Shelley, Jane Austen, and Lewis Carroll to make predictions based on character sequences of lengths 1, 5, 10, and 30.
In this post we will evaluate these networks, comparing the standard method of padding input seeds with the proposed bootstrapping method.

For details on the data wrangling process, see the [Appendix](https://jamesotto852.github.io/text-generation-0/).

***

First, we load several `R` packages necessary for our analysis:

```{r setup_visible, message = FALSE}
library("tidyverse"); theme_set(theme_bw()); theme_update(panel.grid.minor = element_blank())
library("here")

library("tensorflow")
library("keras")
```

```{r start_tensorflow, message = FALSE, include = FALSE}
# Incur the tensorflow start up message before we get to other chunks
tf$constant(1)
```

***

## Making predictions

The following function takes an input sequence of arbitrary length (`seed`), loads the relevant RNN models for a specified author (`author`), 
and generates a number additional characters (`steps`).
It does so either by padding the input seed or via the bootstrapping method, according to the `bootstrap` parameter.

```{r source_create_training_data, echo = FALSE}
source(here("R/run_model.R"), local = knitr::knit_global())
```
```{r show_create_training_data, code=xfun::read_utf8(here::here("R/run_model.R"))}
```

***

### Comparing standard and bootstrapped models

First, we compare the output of the standard and bootstrap methods of predictions.
We do so based on the models trained on Mary Shelley's Frankenstein,
generating 10 sequences of length 100 from the input seed `"A"`.
While neither model is great, subjectively it seems that the bootstrap might be producing slightly more realistic content. 
Especially in the first few characters, the bootstrapped model seems to be more coherent than the padding method.
This makes intuitive sense, the effects of padding would be most pronounced in the first few predictions.

```{r creating_training_data, cache = TRUE, comment = ""}
set.seed(1)

map(1:10, \(.) run_model("A", "Mary Shelley", 100, bootstrap = TRUE)) |>
  walk2(1:10, \(pred, i) cat(i, ": ", pred, "\n\n", sep = ""))
  
map(1:10, \(.) run_model("A", "Mary Shelley", 100, bootstrap = FALSE)) |>
  walk2(1:10, \(pred, i) cat(i, ": ", pred, "\n\n", sep = ""))
```

***

### Comparing models trained on different authors

Below, we include the predictions from the bootstrap models trained on each author 
based on the input seed `"I"`.
Each model seems to have a unique "style", corresponding to the voice in the original texts.
Interestingly, the output from the Jane Austen model seems to be the most coherent --
this is likely due to the fact that her corpus was much larger than the others, consisting of the text from 6 books.

```{r comparing_author_outputs, cache = TRUE, comment = ""}
authors <- list.files(here("Data/Training_Data"))

set.seed(1)
results <- map(authors, run_model, seed = "I", steps = 1000) |>
  (\(x) tibble(output = x))() |>
  mutate(author = authors) |>
  mutate(output = str_replace_all(output, "\\s+", " "))
  
print_results <- function(output, author) {
  cat(author, ":\n\t", output, "\n\n", sep = "")
}

pwalk(results, print_results)
```

***

## Final thoughts

It appears that padding inputs with whitespace may have a negative effect on a model's predictions.
Comparing our models' outputs, if there is a difference it is small.
Note that we chose to train models on sequences of length $k = 1, 5, 10, 30$.
These values were arbitrary, it is likely that performance could be increased with tuning of this hyperparameter.

A challenge in this analysis was the lack of objective measures of quality. 
When comparing the two proposed methods we had to resort to subjective judgements,
comparing the output using our opinions of what qualifies as convincing.
If we had better models, we might be able to compare methods using automated methods such as spell checker.
However, given the quality of the output of the models we trained this would not be an effective measure of comparison.

The code used to conduct this analysis was written to be easy to use, understand, and extend.
It would be very easy to create models for different authors, try different network architectures, or better tune the networks we proposed.
We encourage others to fork [our repository](https://github.com/jamesotto852/Text_Generation) and explore these or other questions!

***

## References








